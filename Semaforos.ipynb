{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhdYJYnIr8IJ/julRwskRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VictorSilvaCamargo/Atividade-Avaliativa-Em-Grupo-RA3-1/blob/main/Semaforos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Atividade Prática Avaliativa Em Grupo - RA3-1\n",
        "Para realizar esta atividade e conseguir os pontos equivalentes na média da RA-3 você deve fazer uma cópia deste notebook, manter inalterado este enunciado, incluir o nome de todos os componentes do grupo e, finalmente resolver o exercício a seguir em uma célula de código: \n",
        "\n",
        "Seu objetivo é escrever um programa Python que recupere o texto de cinco páginas diferentes da Wikipedia. Cada página deve ser recuperada por uma thread diferente.\n",
        "\n",
        "O programa deve cumprir os seguintes requisitos:\n",
        "\n",
        "1. Use a biblioteca requests para realizar requisições HTTP GET para as páginas da Wikipedia.\n",
        "2. Use a biblioteca BeautifulSoup para extrair o texto das páginas da web.\n",
        "3. Crie uma thread para cada página da Wikipedia que você está acessando.\n",
        "4. Cada thread deve escrever o texto extraído no mesmo arquivo de texto compartilhado.\n",
        "5. Garanta que não ocorram condições de corrida ao escrever no arquivo de texto compartilhado.\n",
        "6. O thread que gravou cada texto no arquivo deve ser identificado no próprio texto gravado e em uma impressão no terminal que permita acompanhar o processo de gravação. \n",
        "\n",
        "Lembre-se não podem haver condições de corrida. Use semáforos. \n"
      ],
      "metadata": {
        "id": "FOE2R4ggS4vU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUZrAoXTSzGp",
        "outputId": "b0cecd89-a77a-445c-f47c-9000273cf6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread 1 está enviando uma requisição para a pagina web\n",
            "Thread 1 esta inicializando um objeto BeautifulSoup com o conteúdo da página\n",
            "Thread 1 está gravando o texto da pagina web no arquivo de texto\n",
            "\n",
            "Thread 2 está enviando uma requisição para a pagina web\n",
            "Thread 2 esta inicializando um objeto BeautifulSoup com o conteúdo da página\n",
            "Thread 2 está gravando o texto da pagina web no arquivo de texto\n",
            "\n",
            "Thread 3 está enviando uma requisição para a pagina web\n",
            "Thread 3 esta inicializando um objeto BeautifulSoup com o conteúdo da página\n",
            "Thread 3 está gravando o texto da pagina web no arquivo de texto\n",
            "\n",
            "Thread 4 está enviando uma requisição para a pagina web\n",
            "Thread 4 esta inicializando um objeto BeautifulSoup com o conteúdo da página\n",
            "Thread 4 está gravando o texto da pagina web no arquivo de texto\n",
            "\n",
            "Thread 5 está enviando uma requisição para a pagina web\n",
            "Thread 5 esta inicializando um objeto BeautifulSoup com o conteúdo da página\n",
            "Thread 5 está gravando o texto da pagina web no arquivo de texto\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importando as bibliotecas necessárias\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "semaphore = threading.Semaphore()\n",
        "\n",
        "# A URL da página da web que você quer acessar\n",
        "url1 = \"https://pt.wikipedia.org/wiki/Pinh%C3%A3o\"\n",
        "url2 = \"https://pt.wikipedia.org/wiki/Pontif%C3%ADcia_Universidade_Cat%C3%B3lica_do_Paran%C3%A1\"\n",
        "url3 = \"https://pt.wikipedia.org/wiki/Multitarefa\"\n",
        "url4 = \"https://pt.wikipedia.org/wiki/Cache\"\n",
        "url5 = \"https://pt.wikipedia.org/wiki/Desempenho\"\n",
        "\n",
        "\n",
        "def extrair_html(thread_name, url):\n",
        "    semaphore.acquire()\n",
        "    print(f\"{thread_name} está enviando uma requisição para a pagina web\")\n",
        "    response = requests.get(url)\n",
        "    print(f\"{thread_name} esta inicializando um objeto BeautifulSoup com o conteúdo da página\")\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    print(f\"{thread_name} está gravando o texto da pagina web no arquivo de texto\\n\")\n",
        "    page_text = soup.get_text()\n",
        "    with open('arquivo_texto.txt', 'a') as f:\n",
        "        f.write(\"-\" * 60 + \"\\n\")\n",
        "        f.write(f\"Este texto foi gravado pela {thread_name}:\\n{page_text}\\n\")\n",
        "    semaphore.release()\n",
        "\n",
        "urls = [url1, url2, url3, url4, url5]\n",
        "threads = []\n",
        "for i in range(5):\n",
        "    # Criando uma nova thread\n",
        "    t = threading.Thread(target=extrair_html, args=(f\"Thread {i + 1}\", urls[i]))\n",
        "    threads.append(t)\n",
        "    t.start()  # Iniciando a thread\n",
        "\n",
        "# Esperando todas as threads terminarem antes de mostrar o valor final do contador\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}